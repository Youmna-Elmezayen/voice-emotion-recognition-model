{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython\nimport IPython.display\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.preprocessing import normalize\nimport sklearn.metrics as metrics\nimport seaborn as sn\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T15:01:16.089498Z","iopub.execute_input":"2023-05-21T15:01:16.089902Z","iopub.status.idle":"2023-05-21T15:01:16.098905Z","shell.execute_reply.started":"2023-05-21T15:01:16.089871Z","shell.execute_reply":"2023-05-21T15:01:16.097778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading data and plotting**","metadata":{}},{"cell_type":"code","source":"Crema_Path='/kaggle/input/speech-emotion-recognition-en/Crema'\n\ncrema=[] # list to place our data as label | file path\nfor wav in os.listdir(Crema_Path):\n    emotion=wav.partition(\".wav\")[0].split('_') # the name of the file is as follows (#_chars_label)\n    if emotion[2]=='SAD': # labels are the 3rd element in our splitted list because we split by '_'\n        crema.append(('sad',Crema_Path+'/'+wav))\n    elif emotion[2]=='ANG':\n        crema.append(('angry',Crema_Path+'/'+wav))\n    elif emotion[2]=='DIS':\n        crema.append(('disgust',Crema_Path+'/'+wav))\n    elif emotion[2]=='FEA':\n        crema.append(('fear',Crema_Path+'/'+wav))\n    elif emotion[2]=='HAP':\n        crema.append(('happy',Crema_Path+'/'+wav))\n    elif emotion[2]=='NEU':\n        crema.append(('neutral',Crema_Path+'/'+wav))\n    else:\n        crema.append(('unknown',Crema_Path+'/'+wav))\nCrema_df = pd.DataFrame.from_dict(crema) # convert list to dataframe to be easier to manipulate\nCrema_df.rename(columns={0:'Emotion',1:'File_Path'},inplace=True)\nCrema_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:50:34.357431Z","iopub.execute_input":"2023-05-21T14:50:34.357859Z","iopub.status.idle":"2023-05-21T14:50:34.778414Z","shell.execute_reply.started":"2023-05-21T14:50:34.357828Z","shell.execute_reply":"2023-05-21T14:50:34.777622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors={'disgust':'#804E2D','happy':'#F19C0E','sad':'#478FB8','neutral':'#4CB847','fear':'#7D55AA','angry':'#C00808','surprise':'#EE00FF'}\n\ndef wave_plot(data,sr,emotion,color): # function to display waveform of audio file\n    plt.figure()\n    plt.title(f'{emotion} emotion for waveplot',size=17)\n    plt.ylabel('Amplitude')\n    librosa.display.waveshow(y=data,sr=sr,color=color)\n\nemotion_names = Crema_df['Emotion'].unique() # get the names of emotion classes\n\naudio_path=[] # list to hold paths of the first audio file of each emotion\nfor emotion in emotion_names:\n    path = np.array(Crema_df['File_Path'][Crema_df['Emotion']==emotion])[1] # add path only if emotion is the current emotion\n    data,sr = librosa.load(path)\n    wave_plot(data,sr,emotion,colors[emotion]) # plot waveform of audio\n    audio_path.append(path)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:50:39.584981Z","iopub.execute_input":"2023-05-21T14:50:39.585389Z","iopub.status.idle":"2023-05-21T14:50:55.439821Z","shell.execute_reply.started":"2023-05-21T14:50:39.585360Z","shell.execute_reply":"2023-05-21T14:50:55.438760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[0]} Audio Sample')\nIPython.display.Audio(audio_path[0]) # display audio player (has to be indexed and not in loop)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:03:16.014780Z","iopub.execute_input":"2023-05-21T11:03:16.015220Z","iopub.status.idle":"2023-05-21T11:03:16.035378Z","shell.execute_reply.started":"2023-05-21T11:03:16.015195Z","shell.execute_reply":"2023-05-21T11:03:16.034018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[1]} Audio Sample')\nIPython.display.Audio(audio_path[1])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:03:46.431073Z","iopub.execute_input":"2023-05-21T11:03:46.431497Z","iopub.status.idle":"2023-05-21T11:03:46.442429Z","shell.execute_reply.started":"2023-05-21T11:03:46.431463Z","shell.execute_reply":"2023-05-21T11:03:46.441005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[2]} Audio Sample')\nIPython.display.Audio(audio_path[2])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:03:56.969260Z","iopub.execute_input":"2023-05-21T11:03:56.969597Z","iopub.status.idle":"2023-05-21T11:03:56.982857Z","shell.execute_reply.started":"2023-05-21T11:03:56.969573Z","shell.execute_reply":"2023-05-21T11:03:56.981520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[3]} Audio Sample')\nIPython.display.Audio(audio_path[3])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:18.695559Z","iopub.execute_input":"2023-05-21T11:04:18.696166Z","iopub.status.idle":"2023-05-21T11:04:18.706399Z","shell.execute_reply.started":"2023-05-21T11:04:18.696131Z","shell.execute_reply":"2023-05-21T11:04:18.705097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[4]} Audio Sample')\nIPython.display.Audio(audio_path[4])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:15.810490Z","iopub.execute_input":"2023-05-21T11:04:15.810961Z","iopub.status.idle":"2023-05-21T11:04:15.821800Z","shell.execute_reply.started":"2023-05-21T11:04:15.810910Z","shell.execute_reply":"2023-05-21T11:04:15.820622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{emotion_names[5]} Audio Sample')\nIPython.display.Audio(audio_path[5])","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:28.759547Z","iopub.execute_input":"2023-05-21T11:04:28.759900Z","iopub.status.idle":"2023-05-21T11:04:28.770410Z","shell.execute_reply.started":"2023-05-21T11:04:28.759877Z","shell.execute_reply":"2023-05-21T11:04:28.768980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ml_spectogram(data,sr,emotion):\n    ps = librosa.feature.melspectrogram(y=data, sr=sr) # get spectrogram feature\n    ps_db= librosa.power_to_db(ps, ref=np.max) # convert from power spectrogram to dB\n    plt.figure()\n    plt.title(f'{emotion} emotion for ml spectogram',size=17)\n    librosa.display.specshow(ps_db, x_axis='s', y_axis='log') # plot spectrogram\n    plt.colorbar() # in the colorbar, the brighter in color, the higher the frequency\n\naudio_path=[]\nfor emotion in emotion_names:\n    path = np.array(Crema_df['File_Path'][Crema_df['Emotion']==emotion])[1] # add path only if emotion is the current emotion\n    data,sr = librosa.load(path)\n    ml_spectogram(data,sr,emotion)\n    audio_path.append(path)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:04:45.894807Z","iopub.execute_input":"2023-05-21T11:04:45.895155Z","iopub.status.idle":"2023-05-21T11:04:49.216515Z","shell.execute_reply.started":"2023-05-21T11:04:45.895131Z","shell.execute_reply":"2023-05-21T11:04:49.215016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(crema):\n    max_len = 0\n    index = 0\n    for i in range(crema.shape[0]): # loop over the dataset\n        data,sr = librosa.load(crema[i][1])\n        if data.shape[0] > max_len: # check the largest sized audio file and save its length\n          max_len = data.shape[0]\n          index = i # also save its index\n    return max_len, index\n    # we will use this information to help us in padding the rest of the data to match\ndef pad_data(length, index):\n    emotion = []\n    audio = []\n    sample_rate = []\n    for i in range(crema.shape[0]):\n          data,sr = librosa.load(crema[i][1])\n          padd = length - data.shape[0] # padding size is the difference in max size and current size\n          if i != index: # if the current file is not already the largest, we need padding\n              data = np.pad(data, pad_width= (0, padd)) # pad with 0s(default) with the length from 0 to padd\n          audio.append(data)\n          emotion.append(crema[i][0]) # append data label into emotion to hold all data labels\n          sample_rate.append(sr) # append each file's sample rate\n    return audio, emotion, sample_rate","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:51:01.194406Z","iopub.execute_input":"2023-05-21T14:51:01.195209Z","iopub.status.idle":"2023-05-21T14:51:01.205917Z","shell.execute_reply.started":"2023-05-21T14:51:01.195170Z","shell.execute_reply":"2023-05-21T14:51:01.204680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Load data and get max length of audio**","metadata":{}},{"cell_type":"code","source":"crema = np.array(crema)\nlength, index = load_data(crema) # get max length and index","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:51:05.945684Z","iopub.execute_input":"2023-05-21T14:51:05.946648Z","iopub.status.idle":"2023-05-21T14:53:05.380068Z","shell.execute_reply.started":"2023-05-21T14:51:05.946612Z","shell.execute_reply":"2023-05-21T14:53:05.378994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Padding the data to be of same length**","metadata":{}},{"cell_type":"code","source":"audio, emotion, sample_rate = pad_data(length, index) # add padding and get audio files, labels, and sample rates\naudio = np.array(audio)\nemotion = np.array(emotion)\nsample_rate = np.array(sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:53:10.367603Z","iopub.execute_input":"2023-05-21T14:53:10.367994Z","iopub.status.idle":"2023-05-21T14:53:40.252122Z","shell.execute_reply.started":"2023-05-21T14:53:10.367965Z","shell.execute_reply":"2023-05-21T14:53:40.251211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **One hot encoding for the labels**","metadata":{}},{"cell_type":"code","source":"# hot encoding is used to encode our labels into binary vectors\nunique, inverse = np.unique(emotion, return_inverse=True) # inverse outputs labels as decimal numbers\nemotion = np.eye(unique.shape[0])[inverse] # np.eye outputs hot encoding\nprint(unique)\nprint(inverse)\nprint(emotion)\nprint(emotion.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:54:09.859636Z","iopub.execute_input":"2023-05-21T14:54:09.860073Z","iopub.status.idle":"2023-05-21T14:54:09.870247Z","shell.execute_reply.started":"2023-05-21T14:54:09.860038Z","shell.execute_reply":"2023-05-21T14:54:09.869004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Split the data**","metadata":{}},{"cell_type":"code","source":"# split data into train and test 70:30, then split train into train and validation 95:5\nX_train,X_test,y_train,y_test=train_test_split(audio, emotion, random_state=42, test_size=0.3, stratify=emotion)\nX_t,X_valid,y_t,y_valid=train_test_split(X_train, y_train, random_state=42, test_size=0.05, stratify=y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:54:18.371219Z","iopub.execute_input":"2023-05-21T14:54:18.371602Z","iopub.status.idle":"2023-05-21T14:54:20.435215Z","shell.execute_reply.started":"2023-05-21T14:54:18.371573Z","shell.execute_reply":"2023-05-21T14:54:20.434166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zero-crossing rate is the rate of sign-changes along a signal\n# the librosa function splits data into windows or frames and takes samples within this window\n# and gets average of their sign changes using sgn function\n# hops are skips within a frame to select a number of samples for our average\ndef zcr(data):\n    zcr=librosa.feature.zero_crossing_rate(data,frame_length=2048,hop_length=512)\n    return np.mean(zcr)\n\n# RMSE acts as an indicator of loudness, since higher the energy, louder the sound\n# frames are used to calculate energy over them\n# hops are similar to zcr\ndef rmse(data):\n    rmse=librosa.feature.rms(y=data,frame_length=2048,hop_length=512)\n    return np.mean(rmse)\n\n# concatenate our 1D features (zcr and rmse)\ndef extract_features(data):\n    result=np.array([])\n    result=np.hstack((result, zcr(data), rmse(data)))\n    return result\n\n# get mel spectrogram feature\ndef extract_features_spect(data, sample_rate):\n    return librosa.feature.melspectrogram(y=data, sr=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:54:31.856813Z","iopub.execute_input":"2023-05-21T14:54:31.857215Z","iopub.status.idle":"2023-05-21T14:54:31.865917Z","shell.execute_reply.started":"2023-05-21T14:54:31.857185Z","shell.execute_reply":"2023-05-21T14:54:31.864545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating the feature space**","metadata":{}},{"cell_type":"markdown","source":"## **Extract the zero crossing rate and the energy features**","metadata":{}},{"cell_type":"code","source":"train_audio = np.array([])\nfor i in range(X_train.shape[0]):\n    result = extract_features(X_train[i]) # extract 1D features for train data\n    if i==0:\n        train_audio = np.hstack((train_audio, result))\n    else:\n        train_audio = np.vstack((train_audio, result))\n        \ntest_audio = np.array([])\nfor i in range(X_test.shape[0]): # extract 1D features for test data\n    result = extract_features(X_test[i])\n    if i==0:\n        test_audio = np.hstack((test_audio, result))\n    else:\n        test_audio = np.vstack((test_audio, result))\n        \nvalid_audio = np.array([])\nfor i in range(X_valid.shape[0]): # extract 1D features for validation data\n    if i==0:\n        valid_audio = np.hstack((valid_audio, result))\n    else:\n        valid_audio = np.vstack((valid_audio, result))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:54:36.271710Z","iopub.execute_input":"2023-05-21T14:54:36.272128Z","iopub.status.idle":"2023-05-21T14:55:56.041106Z","shell.execute_reply.started":"2023-05-21T14:54:36.272082Z","shell.execute_reply":"2023-05-21T14:55:56.039992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Extract the ml spectogram feature**","metadata":{}},{"cell_type":"code","source":"train_audio_spec = []\nfor i in range(X_train.shape[0]): # extract 2D features for train data\n    result = extract_features_spect(X_train[i], sample_rate[i])\n    train_audio_spec.append(result)\n\ntrain_audio_spec = np.array(train_audio_spec)\n\ntest_audio_spec = []\nfor i in range(X_test.shape[0]): # extract 2D features for test data\n    result = extract_features_spect(X_test[i], sample_rate[i])\n    test_audio_spec.append(result)\n\ntest_audio_spec = np.array(test_audio_spec)\n\nvalid_audio_spec = []\nfor i in range(X_valid.shape[0]): # extract 2D features for validation data\n    result = extract_features_spect(X_valid[i], sample_rate[i])\n    valid_audio_spec.append(result)\n\nvalid_audio_spec = np.array(valid_audio_spec)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:56:03.798232Z","iopub.execute_input":"2023-05-21T14:56:03.798608Z","iopub.status.idle":"2023-05-21T14:59:13.007508Z","shell.execute_reply.started":"2023-05-21T14:56:03.798578Z","shell.execute_reply":"2023-05-21T14:59:13.005941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Reshaping the data**","metadata":{}},{"cell_type":"code","source":"print(train_audio.shape)\nprint(test_audio.shape)\nprint(valid_audio.shape)\n\n# reshape is used to express the audio data as its features along with the number of channels of sound\ntrain_audio = train_audio.reshape(train_audio.shape[0], train_audio.shape[1], 1) # 1 in 3rd arg denotes channels (mono)\nprint(train_audio.shape)\n\ntest_audio = test_audio.reshape(test_audio.shape[0], test_audio.shape[1], 1)\nprint(test_audio.shape)\n\nvalid_audio = valid_audio.reshape(valid_audio.shape[0], valid_audio.shape[1], 1)\nprint(valid_audio.shape)\n\nprint(train_audio_spec.shape)\nprint(test_audio_spec.shape)\nprint(valid_audio_spec.shape)\n\ntrain_audio_spec = train_audio_spec.reshape(train_audio_spec.shape[0], train_audio_spec.shape[1], train_audio_spec.shape[2], 1)\nprint(train_audio_spec.shape)\n\ntest_audio_spec = test_audio_spec.reshape(test_audio_spec.shape[0], test_audio_spec.shape[1], test_audio_spec.shape[2], 1)\nprint(train_audio_spec.shape)\n\nvalid_audio_spec = valid_audio_spec.reshape(valid_audio_spec.shape[0], valid_audio_spec.shape[1], valid_audio_spec.shape[2], 1)\nprint(train_audio_spec.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:00:16.931096Z","iopub.execute_input":"2023-05-21T15:00:16.931491Z","iopub.status.idle":"2023-05-21T15:00:16.946439Z","shell.execute_reply.started":"2023-05-21T15:00:16.931454Z","shell.execute_reply":"2023-05-21T15:00:16.944799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CNN model for zcr and energy features**","metadata":{}},{"cell_type":"code","source":"# run conv1D model with different number of filters (32, 64, 128)\n# filter size = 7, uses padding of zeros, and activation function as relu function\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv1D(32, kernel_size=7, strides=1,padding='same', activation='relu', input_shape=(train_audio.shape[1],1)))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')) # maxpooling with stride(step) = 2\nmodel.add(tf.keras.layers.Conv1D(64, kernel_size=7, strides=1,padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=7, strides=1,padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation='relu')) # Dense layer feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer. \nmodel.add(tf.keras.layers.Dense(6, activation='softmax'))\n\nprint(model.summary())\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n\nhistory = model.fit(train_audio, y_train, epochs=40, validation_data=(valid_audio, y_valid)) # run model on data","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:18:01.542906Z","iopub.execute_input":"2023-05-21T15:18:01.543284Z","iopub.status.idle":"2023-05-21T15:18:43.867169Z","shell.execute_reply.started":"2023-05-21T15:18:01.543256Z","shell.execute_reply":"2023-05-21T15:18:43.866020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:18:43.869957Z","iopub.execute_input":"2023-05-21T15:18:43.870281Z","iopub.status.idle":"2023-05-21T15:18:44.155893Z","shell.execute_reply.started":"2023-05-21T15:18:43.870253Z","shell.execute_reply":"2023-05-21T15:18:44.154771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:19:05.443703Z","iopub.execute_input":"2023-05-21T15:19:05.444134Z","iopub.status.idle":"2023-05-21T15:19:05.682226Z","shell.execute_reply.started":"2023-05-21T15:19:05.444101Z","shell.execute_reply":"2023-05-21T15:19:05.680809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_audio, y_test, verbose=2) # get accuracy and loss percentages of model\nprint('Accuracy: ', test_acc)\nprint('Loss: ', test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:19:07.992742Z","iopub.execute_input":"2023-05-21T15:19:07.993826Z","iopub.status.idle":"2023-05-21T15:19:08.195418Z","shell.execute_reply.started":"2023-05-21T15:19:07.993781Z","shell.execute_reply":"2023-05-21T15:19:08.194388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_audio) # test our model with test data\n\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1))\ndf_cm = pd.DataFrame(cm, index=[i for i in emotion_names],columns=[i for i in emotion_names])\nplt.figure()\nsn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:19:10.116964Z","iopub.execute_input":"2023-05-21T15:19:10.117402Z","iopub.status.idle":"2023-05-21T15:19:10.820455Z","shell.execute_reply.started":"2023-05-21T15:19:10.117371Z","shell.execute_reply":"2023-05-21T15:19:10.819636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1),average='weighted')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:19:18.543936Z","iopub.execute_input":"2023-05-21T15:19:18.544332Z","iopub.status.idle":"2023-05-21T15:19:18.556346Z","shell.execute_reply.started":"2023-05-21T15:19:18.544303Z","shell.execute_reply":"2023-05-21T15:19:18.555020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run another conv1D model with different number of filters (64, 128)\n# filter size = 3, uses padding of zeros, and activation function as relu function\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv1D(64, kernel_size=3, strides=1,padding='same', activation='relu', input_shape=(train_audio.shape[1],1)))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Conv1D(128, kernel_size=3, strides=1,padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(6, activation='softmax'))\n\nprint(model.summary())\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n\nhistory = model.fit(train_audio, y_train, epochs=50, validation_data=(valid_audio, y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:32:35.174013Z","iopub.execute_input":"2023-05-21T12:32:35.174382Z","iopub.status.idle":"2023-05-21T12:33:03.461577Z","shell.execute_reply.started":"2023-05-21T12:32:35.174357Z","shell.execute_reply":"2023-05-21T12:33:03.460891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:33:16.514001Z","iopub.execute_input":"2023-05-21T12:33:16.514387Z","iopub.status.idle":"2023-05-21T12:33:16.717068Z","shell.execute_reply.started":"2023-05-21T12:33:16.514359Z","shell.execute_reply":"2023-05-21T12:33:16.715455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:33:44.578208Z","iopub.execute_input":"2023-05-21T12:33:44.579465Z","iopub.status.idle":"2023-05-21T12:33:44.783805Z","shell.execute_reply.started":"2023-05-21T12:33:44.579418Z","shell.execute_reply":"2023-05-21T12:33:44.782222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_audio, y_test, verbose=2)\nprint('Accuracy: ', test_acc)\nprint('Loss: ', test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:34:26.555398Z","iopub.execute_input":"2023-05-21T12:34:26.555761Z","iopub.status.idle":"2023-05-21T12:34:26.766799Z","shell.execute_reply.started":"2023-05-21T12:34:26.555733Z","shell.execute_reply":"2023-05-21T12:34:26.764229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_audio)\n\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1))\ndf_cm = pd.DataFrame(cm, index=[i for i in emotion_names],columns=[i for i in emotion_names])\nplt.figure()\nsn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:34:30.202006Z","iopub.execute_input":"2023-05-21T12:34:30.202396Z","iopub.status.idle":"2023-05-21T12:34:30.814555Z","shell.execute_reply.started":"2023-05-21T12:34:30.202363Z","shell.execute_reply":"2023-05-21T12:34:30.813183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CNN model for ml spectogram feature**","metadata":{}},{"cell_type":"code","source":"# run conv2D model with different number of filters (32, 64)\n# filter size = (3,3), uses padding of zeros, and activation function as relu function\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1),padding='same', activation='relu', input_shape=(train_audio_spec.shape[1], train_audio_spec.shape[2], 1)))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1),padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(6, activation='softmax'))\n\nprint(model.summary())\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n\nhistory = model.fit(train_audio_spec, y_train, epochs=20, validation_data=(valid_audio_spec, y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:13:34.528920Z","iopub.execute_input":"2023-05-21T11:13:34.529289Z","iopub.status.idle":"2023-05-21T11:46:58.798601Z","shell.execute_reply.started":"2023-05-21T11:13:34.529260Z","shell.execute_reply":"2023-05-21T11:46:58.795700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:47:28.537453Z","iopub.execute_input":"2023-05-21T11:47:28.537942Z","iopub.status.idle":"2023-05-21T11:47:28.794409Z","shell.execute_reply.started":"2023-05-21T11:47:28.537905Z","shell.execute_reply":"2023-05-21T11:47:28.792720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:47:33.810032Z","iopub.execute_input":"2023-05-21T11:47:33.811197Z","iopub.status.idle":"2023-05-21T11:47:34.011065Z","shell.execute_reply.started":"2023-05-21T11:47:33.811145Z","shell.execute_reply":"2023-05-21T11:47:34.009588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_audio_spec, y_test, verbose=2)\nprint('Accuracy: ', test_acc)\nprint('Loss: ', test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:54:18.791925Z","iopub.execute_input":"2023-05-21T11:54:18.792287Z","iopub.status.idle":"2023-05-21T11:54:28.769402Z","shell.execute_reply.started":"2023-05-21T11:54:18.792260Z","shell.execute_reply":"2023-05-21T11:54:28.768234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_audio_spec)\n\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1))\ndf_cm = pd.DataFrame(cm, index=[i for i in emotion_names],columns=[i for i in emotion_names])\nplt.figure()\nsn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:48:54.778408Z","iopub.execute_input":"2023-05-21T11:48:54.778764Z","iopub.status.idle":"2023-05-21T11:49:05.542981Z","shell.execute_reply.started":"2023-05-21T11:48:54.778739Z","shell.execute_reply":"2023-05-21T11:49:05.542063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1),average='weighted')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T11:49:16.884694Z","iopub.execute_input":"2023-05-21T11:49:16.885098Z","iopub.status.idle":"2023-05-21T11:49:16.898192Z","shell.execute_reply.started":"2023-05-21T11:49:16.885072Z","shell.execute_reply":"2023-05-21T11:49:16.896170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Comparison between conv1D and conv2D**","metadata":{}},{"cell_type":"markdown","source":"### **conv1D with architecture of conv2D**","metadata":{}},{"cell_type":"code","source":"# to compare between 1D and 2D models, we will try to use the same parameters of our 2D model in a 1D model\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv1D(32, kernel_size=3, strides=1,padding='same', activation='relu', input_shape=(train_audio.shape[1],1)))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Conv1D(64, kernel_size=3, strides=1,padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same'))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(6, activation='softmax'))\n\nprint(model.summary())\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n\nhistory = model.fit(train_audio, y_train, epochs=20, validation_data=(valid_audio, y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:08:08.759885Z","iopub.execute_input":"2023-05-21T15:08:08.760295Z","iopub.status.idle":"2023-05-21T15:08:17.865757Z","shell.execute_reply.started":"2023-05-21T15:08:08.760264Z","shell.execute_reply":"2023-05-21T15:08:17.864785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:08:57.903178Z","iopub.execute_input":"2023-05-21T15:08:57.903556Z","iopub.status.idle":"2023-05-21T15:08:58.201692Z","shell.execute_reply.started":"2023-05-21T15:08:57.903526Z","shell.execute_reply":"2023-05-21T15:08:58.200648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:09:01.320493Z","iopub.execute_input":"2023-05-21T15:09:01.321576Z","iopub.status.idle":"2023-05-21T15:09:01.613377Z","shell.execute_reply.started":"2023-05-21T15:09:01.321539Z","shell.execute_reply":"2023-05-21T15:09:01.612124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_audio, y_test, verbose=2)\nprint('Accuracy: ', test_acc)\nprint('Loss: ', test_loss)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:09:26.905742Z","iopub.execute_input":"2023-05-21T15:09:26.906138Z","iopub.status.idle":"2023-05-21T15:09:27.084512Z","shell.execute_reply.started":"2023-05-21T15:09:26.906107Z","shell.execute_reply":"2023-05-21T15:09:27.083374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_audio)\n\ncm = metrics.confusion_matrix(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1))\ndf_cm = pd.DataFrame(cm, index=[i for i in emotion_names],columns=[i for i in emotion_names])\nplt.figure()\nsn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:09:49.695130Z","iopub.execute_input":"2023-05-21T15:09:49.695614Z","iopub.status.idle":"2023-05-21T15:09:50.399028Z","shell.execute_reply.started":"2023-05-21T15:09:49.695578Z","shell.execute_reply":"2023-05-21T15:09:50.397853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(np.argmax(y_test,axis=-1), np.argmax(y_pred,axis=-1),average='weighted')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:10:20.182326Z","iopub.execute_input":"2023-05-21T15:10:20.182749Z","iopub.status.idle":"2023-05-21T15:10:20.194290Z","shell.execute_reply.started":"2023-05-21T15:10:20.182702Z","shell.execute_reply":"2023-05-21T15:10:20.193160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **conv2D with architecture of conv1D**","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1),padding='same', activation='relu', input_shape=(train_audio_spec.shape[1], train_audio_spec.shape[2], 1)))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\nmodel.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1),padding='same', activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(6, activation='softmax'))\n\nprint(model.summary())\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')\n\nhistory = model.fit(train_audio_spec, y_train, epochs=20, validation_data=(valid_audio_spec, y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:21:35.586551Z","iopub.execute_input":"2023-05-21T15:21:35.586954Z","iopub.status.idle":"2023-05-21T15:21:47.897110Z","shell.execute_reply.started":"2023-05-21T15:21:35.586924Z","shell.execute_reply":"2023-05-21T15:21:47.893816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Paper On Speech Emotion Recognition in Neurological Disorders Using Convolutional Neural Network\n\nThis paper is concerned with the detection of emotion in people that suffer a neurological disorder that would make it typically difficult for them to *express* emotion. The proposed SER model is claimed to have helped detect and classify emotions (happiness, calmness, fear, and more) in patients. The system uses tonal properties like **MFCCs** and RAVDESS audio speech and song databases for training and testing. In addition, a custom local dataset is developed to support further training and testing. \n\nMFCC is an acronym for Mel Frequency Cepstral Co-efficients which are the coefficients that collectively make up an MFC. MFC is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. Mel scale is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear.\nA frequency measured in Hertz (f) can be converted to the Mel scale using the following formula:\n\n               Mel(f) = 2595log(1 + f/700)\n\nThis system enables one to classify eight emotions of neurologically disordered person including calm, angry, fearful, disgust, happy, surprise, neutral and sad. \n\nThis paper's methodolgy is the following: \n\n1- The data is taken from RAVDESS dataset\\\n2- The data is preprocessed\\\n3- The features are extracted from the data\\\n4- The model is used on the data\\\n5- The data is classified\n\nNow let's walk through each step and explain briefly what it does:\n ### 1- RAVDESS and local datasets\nThe Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is a validated database of emotional speech and song. It contains 7356 files including 8 emotions such as anger, happiness, calm, neutral, surprise, sad, fear, and disgust. It has speech and song files under three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4) and Video-only (no sound). All the recordings are in American English.\\\nWe take in this dataset using its path on the device and we read it into a data structure *(ex: list)*. We then use the label given in the name of the file to split data into their respective emotion class *(ex: 'ANG' in filename denotes angry class)*\\\nThe local dataset is created by recording voices from 25 patients from Chittagong, Bangladesh. Ten of them are stroke patients, eight of them are affected with dementia, four of them have epilepsy and rest of them have migraine headache. There are 400 audio files in 8 emotions.\n### 2- Preprocessing\nAll files are used with a sampling rate of 16KHz using the parameter *‘sr = 16000’* in the *load function of the Librosa library*. Augmentation in the audio database usually generates additional audio files by applying some special operation on the original database, such as injecting noise, adjusting pitch, changing vocal tract, adjusting speed, etc. In this case, all of the files are augmented with *injecting noise by using the NoiseAug function from the nlpaug library*.\n### 3- Feature extraction\nFor feature extraction, the *Mfcc function of the Librosa library* is used. The sample rate is 16KHz for each audio file. The number of *MFCC extracted are 100*. The shape of the extracted features would not be the same and the range would not be specific without normalization. The unstructured feature may reduce the accuracy and recognition rate. In this research, after extracting features from each file, we normalized them by *subtracting each feature from the maximum one* to make the shape the same. After normalization, these data are used to train and test the system.\n### 4- CNN model\nThe normalised data is then fed into the proposed model for emotion prediction. There are four convolution layers in this model with 16, 32, 64, and 128 filters and the kernel size for each layer is 2*2. Using keras API, we can use the *conv2D function* to build our model with desired number of filters and kernel size. Rectified Linear Unit (ReLU) used as the activation function in each convolution layer as shown.\n```math\n               ReLU(y) = max(0,y)\n```\nAfter the convolution layer, there is a max-pooling layer where the pool size is 2*2. It selects the largest value from the rectified feature map and reduces the size of the data, so the number of parameters is decreased. Here, *MaxPooling from keras API* is also used. Like the convolution layer, ReLU has been applied as an activation function in hidden layers. A dropout layer is also inserted with the dropout value of 0.2 which randomly deactivates 20% neurons to avoid over-fitting. In the last hidden layer, one Global Average Pooling layer has been added which takes the average which is suitable for feeding into our dense output layer. The output layer of this model consists of eight nodes as it has eight classes. As an activation function, Softmax has been applied as shown in this layer.\n```math\n                Softmax(y)=ei∑jej\n```\nThe dataset was split into training set, validation set and testing set using *train_test_split from sklearn*. Training set and validation set were used to train the model. Testing set was used to test the performance of the model. This model was trained using multiple split ratios (70:20:10, 75:15:10, 80:10:10) and activation functions (relu, sigmoid, softmax, softplus). During the learning process, the performance of this model was best when 75:15:10 split ratio and softmax activation function.\n\n### Results\nThe best accuracy for training and validation was 0.937 and 0.825. The average testing accuracy was 0.813 where average training and validation accuracy was 0.913 and 0.817. Confusion matrix of RAVDESS augmented dataset with this best result is shown.\\\n[table.html](https://link.springer.com/chapter/10.1007/978-3-030-59277-6_26/tables/3)\\\nThe best testing accuracy for the local dataset was 0.612. The best accuracy for training and validation was 0.685 and 0.625. The average testing accuracy was 0.610 where average training and validation accuracy was 0.680 and 0.619. The confusion matrix of the local augmented dataset with this best result is presented.\\\n[table.html](https://link.springer.com/chapter/10.1007/978-3-030-59277-6_26/tables/3)\n\n*Numpy library was used for numerical analysis. Matplotlib library was used for graphical representation, such as confusion matrix, accuracy vs epochs graph, loss vs epochs graph, etc.*","metadata":{}}]}